{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа №2, часть 1 (Елизавета Клыкова)#\n",
    "В первую очередь вызовем проверку на PEP-8, затем импортируем все модули, которые пригодятся нам в работе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --user --upgrade pip\n",
    "# ! pip install pycodestyle flake8 pycodestyle_magic\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on\n",
    "# ! pip install pymorphy2\n",
    "# ! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from pprint import pprint\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "# from pymystem3 import Mystem\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# m = Mystem()\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 1 ##\n",
    "Для работы был выбран роман Е.И. Замятина \"Мы\". Файл с текстом называется \"hw2_txt.txt\"; он должен быть расположен в одной директории с данной программой. Считаем файл и запишем его содержимое в переменную text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw2_txt.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 2 ##\n",
    "Обработаем текст с помощью консольного Mystem (mystem.exe, файл с программой и файл с исходным текстом должны лежать в одной папке! Там же окажется созданный файл). Распарсим текст и запишем результат в файл \"mystem.json\" (вместе со знаками препинания и пробелоподобными символами, каждый объект на новой строке)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(r\".\\mystem.exe -c -n -i --format json hw2_txt.txt mystem.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание.** А вот так можно сделать то же самое, замерив время работы. У меня после этого перестают работать все time, расположенные ниже, и все принты. Я пыталась с этим бороться, но проиграла, и Вы просили напомнить об этом в комментариях:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:80: E501 line too long (81 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# %time os.system(r\".\\mystem.exe -c -n -i --format json hw2_txt.txt mystem.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 3 ##\n",
    "Токенизируем текст с помощью NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое с замером времени работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем слова с помощью Pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_words = [morph.parse(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое с замером времени работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time parsed_words = [morph.parse(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сохраним полученный разбор в json. Удобнее всего было бы создать словарь, в котором ключами являлись бы отдельные словоформы, а значениями - варианты их разбора (тоже в виде вложенных словарей). Но в словаре не может быть одинаковых ключей, а в тексте со 100%-ной вероятностью некоторые словоформы встретятся более одного раза. Тогда можно создать список словарей, каждый из которых содержит ключ - слово и значение - множество вариантов разбора. В итоге мы получим структуру такого вида:\n",
    "\n",
    "[{\n",
    "  \n",
    "\t\"словоформа\": {\n",
    "\t\t\"вариант разбора n\": {\n",
    "\t\t\t\"tags\": {\"full_tags\": \"все теги\",\n",
    "                     \"POS\": \"часть речи\",\n",
    "                      ...\n",
    "                     \"voice\": \"залог\"\n",
    "            },\n",
    "\t\t\t\"normal_form\": \"начальная форма\",\n",
    "\t\t\t\"score\": \"вероятность\"\n",
    "\t\t}\n",
    "        {\n",
    "        \"вариант разбора n+1\": {\n",
    "\t\t\t\"tags\": {\"full_tags\": \"все теги\",\n",
    "                     \"POS\": \"часть речи\",\n",
    "                      ...\n",
    "                     \"voice\": \"залог\"\n",
    "            },\n",
    "\t\t\t\"normal_form\": \"начальная форма\",\n",
    "\t\t\t\"score\": \"вероятность\"\n",
    "\t\t}\n",
    "\t}\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_json = []\n",
    "for word in parsed_words:\n",
    "    json_element = OrderedDict()\n",
    "    parsed_word = OrderedDict()\n",
    "    for number, option in enumerate(word):\n",
    "        opt_num = \"option_\" + str(number + 1)\n",
    "        parsed_option = OrderedDict()\n",
    "        tags = OrderedDict()\n",
    "        if \"PNCT\" in str(option.tag):\n",
    "            tags[\"POS\"] = \"PNCT\"\n",
    "        else:\n",
    "            tags[\"full_tags\"] = str(option.tag)\n",
    "            tags[\"POS\"] = str(option.tag.POS)\n",
    "            tags[\"animacy\"] = str(option.tag.animacy)\n",
    "            tags[\"aspect\"] = str(option.tag.aspect)\n",
    "            tags[\"case\"] = str(option.tag.case)\n",
    "            tags[\"gender\"] = str(option.tag.gender)\n",
    "            tags[\"involvement\"] = str(option.tag.involvement)\n",
    "            tags[\"mood\"] = str(option.tag.mood)\n",
    "            tags[\"number\"] = str(option.tag.number)\n",
    "            tags[\"person\"] = str(option.tag.person)\n",
    "            tags[\"tense\"] = str(option.tag.tense)\n",
    "            tags[\"transitivity\"] = str(option.tag.transitivity)\n",
    "            tags[\"voice\"] = str(option.tag.voice)\n",
    "        parsed_option[\"tags\"] = tags\n",
    "        parsed_option[\"normal_form\"] = str(option.normal_form)\n",
    "        parsed_option[\"score\"] = str(option.score)\n",
    "        parsed_word[opt_num] = parsed_option\n",
    "        word_form = str(option.word)\n",
    "    json_element[word_form] = parsed_word\n",
    "    parsed_json.append(json_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем полученный список словарей в новый файл \"pymorphy.json\" с помощью json.dump. Поскольку файл получается больше 25Мб и через Upload files в браузере его загрузить нельзя, вот ссылка на Google-диске: https://drive.google.com/file/d/1rHMYOHwQ5yJjjv637B8LOj7b9u3a7y53/view?usp=sharing. А еще архив с этим файлом тоже лежит в репозитории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pymorphy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(parsed_json, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 4 ##\n",
    "#### Вопрос 1 ###\n",
    "Выясним, какую долю слов составляет каждая часть речи в данном тексте. Чтобы найти общее число слов в тексте, придется взять не длину списка parsed_json (поскольку там присутствуют и знаки препинания тоже), а количество всех словоформ, у которых значением POS является не PNCT.\n",
    "\n",
    "Для каждой словоформы узнаем часть речи (будем рассматривать только первый, наиболее вероятный вариант разбора). Соберем все полученные части речи (кроме PNCT) в список parts_of_speech и создадим на его основе частотный словарь pos_counter.\n",
    "\n",
    "Заодно создадим список всех лемм слов (элементов, не являющихся знаками препинания) для пункта 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = []\n",
    "word_lemmas = []\n",
    "for element in parsed_json:\n",
    "    word = (list(element.keys()))[0]\n",
    "    part_of_speech = element[word][\"option_1\"][\"tags\"][\"POS\"]\n",
    "    if part_of_speech != \"PNCT\":\n",
    "        parts_of_speech.append(part_of_speech)\n",
    "        word_lemmas.append(element[word][\"option_1\"][\"normal_form\"])\n",
    "word_amount = len(parts_of_speech)  # число собственно слов\n",
    "pos_counter = collections.Counter(parts_of_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем словарь pos_counter по значениям в обратном порядке, вычислим долю каждой части речи в данном тексте и выведем полученную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля разных частей речи в данном тексте:\n",
      "NOUN: 0.209\n",
      "ADJF: 0.134\n",
      "VERB: 0.11\n",
      "CONJ: 0.103\n",
      "NPRO: 0.1\n",
      "PREP: 0.096\n",
      "ADVB: 0.084\n",
      "PRCL: 0.067\n",
      "None: 0.028\n",
      "INFN: 0.019\n",
      "PRTF: 0.013\n",
      "ADJS: 0.01\n",
      "PRED: 0.007\n",
      "COMP: 0.006\n",
      "GRND: 0.005\n",
      "NUMR: 0.004\n",
      "PRTS: 0.004\n",
      "INTJ: 0.002\n"
     ]
    }
   ],
   "source": [
    "parts = []\n",
    "for key in sorted(pos_counter, key=pos_counter.get, reverse=True):\n",
    "    part = round(pos_counter[key] / word_amount, 3)\n",
    "    some_POS = str(key) + \": \" + str(part)\n",
    "    parts.append(some_POS)\n",
    "print(\"Доля разных частей речи в данном тексте:\")\n",
    "for pos in parts:\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 2 ####\n",
    "Теперь найдем топ-20 по частотности глаголов и наречий. Для этого сначала запишем все начальные формы глаголов в список common_verbs, а наречий - в common_adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_verbs = []\n",
    "common_adverbs = []\n",
    "for element in parsed_json:\n",
    "    word = (list(element.keys()))[0]\n",
    "    part_of_speech = element[word][\"option_1\"][\"tags\"][\"POS\"]\n",
    "    if part_of_speech == \"VERB\":\n",
    "        common_verbs.append(element[word][\"option_1\"][\"normal_form\"])\n",
    "    elif part_of_speech == \"ADVB\":\n",
    "        common_adverbs.append(element[word][\"option_1\"][\"normal_form\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем на основе каждого из списков создадим частотный словарь, отсортируем по значениям в обратном порядке и выведем первые 20 элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 наиболее частотных глаголов:\n",
      "быть - 442 вхождений\n",
      "мочь - 226 вхождений\n",
      "знать - 161 вхождений\n",
      "видеть - 114 вхождений\n",
      "сказать - 80 вхождений\n",
      "понимать - 78 вхождений\n",
      "хотеть - 76 вхождений\n",
      "говорить - 64 вхождений\n",
      "чувствовать - 55 вхождений\n",
      "увидеть - 54 вхождений\n",
      "идти - 51 вхождений\n",
      "слышать - 47 вхождений\n",
      "помнить - 46 вхождений\n",
      "думать - 39 вхождений\n",
      "стать - 39 вхождений\n",
      "понять - 31 вхождений\n",
      "смотреть - 29 вхождений\n",
      "сидеть - 23 вхождений\n",
      "нумереть - 22 вхождений\n",
      "остановиться - 22 вхождений\n",
      "\n",
      "Топ-20 наиболее частотных наречий:\n",
      "уже - 146 вхождений\n",
      "там - 138 вхождений\n",
      "сейчас - 130 вхождений\n",
      "ясно - 82 вхождений\n",
      "вдруг - 80 вхождений\n",
      "потому - 77 вхождений\n",
      "теперь - 64 вхождений\n",
      "потом - 62 вхождений\n",
      "здесь - 56 вхождений\n",
      "очень - 56 вхождений\n",
      "вниз - 54 вхождений\n",
      "опять - 51 вхождений\n",
      "медленно - 50 вхождений\n",
      "сегодня - 49 вхождений\n",
      "тут - 49 вхождений\n",
      "тогда - 48 вхождений\n",
      "где - 45 вхождений\n",
      "совершенно - 45 вхождений\n",
      "всего - 44 вхождений\n",
      "вверх - 44 вхождений\n"
     ]
    }
   ],
   "source": [
    "common_v = collections.Counter(common_verbs)\n",
    "common_adv = collections.Counter(common_adverbs)\n",
    "\n",
    "print(\"Топ-20 наиболее частотных глаголов:\")\n",
    "for i, verb in enumerate(sorted(common_v, key=common_v.get, reverse=True)):\n",
    "    if i < 20:\n",
    "        print(verb, \"-\", common_v[verb], \"вхождений\")\n",
    "\n",
    "print(\"\\nТоп-20 наиболее частотных наречий:\")\n",
    "for j, adv in enumerate(sorted(common_adv, key=common_adv.get, reverse=True)):\n",
    "    if j < 20:\n",
    "        print(adv, \"-\", common_adv[adv], \"вхождений\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пункт 5 ##\n",
    "Используем список лемм (word_lemmas), полученный при работе Pymorphy (т.к. в NLTK нет лемматизации для русского языка). С помощью nltk.bigrams() и nltk.trigrams() создадим списки биграмм и триграмм соответственно. Затем сформируем частотный словарь по каждому из списков, отсортируем по значениям в обратном порядке и выведем топ-25 элементов.\n",
    "\n",
    "Чтобы не выполнять одно и то же действие дважды, объявим функцию ngrams и применим ее сначала к списку биграмм, затем к списку триграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(ngram_list):\n",
    "    ngram_dict = collections.Counter(ngram_list)\n",
    "    top25_list = []\n",
    "    for i, gram in enumerate(sorted(ngram_dict, key=ngram_dict.get,\n",
    "                                    reverse=True)):\n",
    "        if i < 25:\n",
    "            line = str(gram) + \" - \" + str(ngram_dict[gram]) + \" вхождений\"\n",
    "            top25_list.append(line)\n",
    "    return top25_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-25 биграмм:\n",
      "('и', 'я') - 134 вхождений\n",
      "('я', 'не') - 110 вхождений\n",
      "('у', 'я') - 92 вхождений\n",
      "('мочь', 'быть') - 91 вхождений\n",
      "('не', 'мочь') - 80 вхождений\n",
      "('что', 'я') - 71 вхождений\n",
      "('не', 'знать') - 71 вхождений\n",
      "('это', 'быть') - 69 вхождений\n",
      "('и', 'весь') - 66 вхождений\n",
      "('потому', 'что') - 63 вхождений\n",
      "('и', 'вот') - 61 вхождений\n",
      "('весь', 'это') - 60 вхождений\n",
      "('я', 'быть') - 59 вхождений\n",
      "('в', 'я') - 55 вхождений\n",
      "('единый', 'государство') - 49 вхождений\n",
      "('и', 'не') - 48 вхождений\n",
      "('я', 'в') - 47 вхождений\n",
      "('если', 'бы') - 46 вхождений\n",
      "('не', 'быть') - 45 вхождений\n",
      "('я', 'и') - 45 вхождений\n",
      "('я', 'видеть') - 44 вхождений\n",
      "('я', 'знать') - 43 вхождений\n",
      "('и', 'в') - 43 вхождений\n",
      "('так', 'же') - 41 вхождений\n",
      "('на', 'я') - 40 вхождений\n",
      "\n",
      "Топ-25 триграмм:\n",
      "('я', 'не', 'мочь') - 23 вхождений\n",
      "('один', 'и', 'тот') - 13 вхождений\n",
      "('и', 'тот', 'же') - 13 вхождений\n",
      "('я', 'не', 'знать') - 13 вхождений\n",
      "('так', 'же', 'как') - 12 вхождений\n",
      "('в', 'самый', 'дело') - 12 вхождений\n",
      "('и', 'тотчас', 'же') - 11 вхождений\n",
      "('у', 'я', 'в') - 11 вхождений\n",
      "('до', 'сей', 'пора') - 11 вхождений\n",
      "('если', 'бы', 'я') - 11 вхождений\n",
      "('и', 'я', 'не') - 10 вхождений\n",
      "('один', 'и', 'то') - 9 вхождений\n",
      "('и', 'то', 'же') - 9 вхождений\n",
      "('или', 'мочь', 'быть') - 9 вхождений\n",
      "('в', 'древний', 'дом') - 9 вхождений\n",
      "('там', 'за', 'стена') - 9 вхождений\n",
      "('не', 'знать', 'что') - 9 вхождений\n",
      "('я', 'не', 'хотеть') - 9 вхождений\n",
      "('потому', 'что', 'я') - 9 вхождений\n",
      "('и', 'я', 'видеть') - 9 вхождений\n",
      "('раз', 'в', 'жизнь') - 8 вхождений\n",
      "('у', 'я', 'быть') - 8 вхождений\n",
      "('в', 'один', 'и') - 8 вхождений\n",
      "('и', 'не', 'мочь') - 8 вхождений\n",
      "('вы', 'неведомый', 'мой') - 8 вхождений\n"
     ]
    }
   ],
   "source": [
    "bigram_list = list(nltk.bigrams(word_lemmas))\n",
    "trigram_list = list(nltk.trigrams(word_lemmas))\n",
    "\n",
    "top_bigrams = ngrams(bigram_list)\n",
    "print(\"Топ-25 биграмм:\")\n",
    "for gram in top_bigrams:\n",
    "    print(gram)\n",
    "\n",
    "top_trigrams = ngrams(trigram_list)\n",
    "print(\"\\nТоп-25 триграмм:\")\n",
    "for gram in top_trigrams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему же биграммы и триграммы получились именно такими? С одной стороны, в топ-25 попали сочетания многих очень частотных слов (тех самых, которые являются стоп-словами в NLTK). С другой стороны, некоторые сочетания отражают тематику выбранного текста (\"единый государство\", \"до сей пора\", \"в древний дом\", \"там за стена\" и др.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
